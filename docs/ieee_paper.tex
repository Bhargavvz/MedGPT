\documentclass[conference]{IEEEtran}

% ============================================
% PACKAGES
% ============================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{url}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{balance}
\usepackage{xcolor}
\usepackage{subcaption}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    urlcolor=black
}

% ============================================
% TITLE
% ============================================
\title{Knowledge-Guided Explainable Transformer for Medical Visual Question Answering}

% ============================================
% AUTHORS
% ============================================
\author{
\IEEEauthorblockN{Dr. G. Sharada\IEEEauthorrefmark{1}, P Nandini Reddy\IEEEauthorrefmark{1}, P Anagha Sri Meghana\IEEEauthorrefmark{1}, Dr. S. Siva Skandha\IEEEauthorrefmark{2}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Department of Computer Science and Engineering\\
CMR College of Engineering \& Technology\\
Kandlakoya, Medchal, Hyderabad -- 501401, India\\
Email: \{22H51A05B5, 22H51A05B7\}@cmrcet.ac.in}
\IEEEauthorblockA{\IEEEauthorrefmark{2}Faculty, Department of Computer Science \& Engineering\\
CMR College of Engineering \& Technology\\
Kandlakoya, Medchal, Hyderabad -- 501401, India}
}

\begin{document}

\maketitle

% ============================================
% ABSTRACT
% ============================================
\begin{abstract}
Medical Visual Question Answering (Med-VQA) represents a critical intersection of computer vision and natural language processing, enabling automated analysis and interpretation of medical images through natural language queries. Current approaches face significant limitations: inadequate integration of domain-specific medical knowledge, limited reasoning capabilities for complex clinical questions, and absence of explainability mechanisms essential for clinical adoption. This research presents a Knowledge-Guided Explainable Transformer (KGET) framework that addresses these challenges through three key innovations. First, we integrate structured medical knowledge from UMLS and RadLex ontologies with vision-language representations using a novel cross-attention fusion mechanism with learnable knowledge gating. Second, we employ Qwen2-VL as the backbone with parameter-efficient LoRA fine-tuning, enabling effective adaptation to the medical domain while maintaining computational efficiency. Third, we introduce an explanation generation module that produces clinically relevant rationales alongside predictions, enhancing transparency and trustworthiness. Comprehensive experiments on VQA-RAD, SLAKE, and PathVQA benchmarks demonstrate that KGET achieves state-of-the-art performance with 78.4\% accuracy on VQA-RAD and 76.2\% on SLAKE, representing improvements of 4.2\% and 3.8\% over prior methods respectively. Ablation studies confirm the significance of knowledge integration, while qualitative analysis validates the clinical relevance of generated explanations. The proposed framework advances Medical VQA toward reliable deployment as an AI-assisted clinical decision support system.
\end{abstract}

% ============================================
% KEYWORDS
% ============================================
\begin{IEEEkeywords}
Medical Visual Question Answering, Knowledge-Guided Learning, Explainable Transformer, Vision--Language Models, Medical Knowledge Integration, Multimodal Deep Learning, Clinical Decision Support
\end{IEEEkeywords}

% ============================================
% I. INTRODUCTION
% ============================================
\section{Introduction}

Medical imaging constitutes a vital component of contemporary healthcare, enabling clinicians to identify and diagnose diseases, monitor their progression, and implement treatment strategies using diverse imaging modalities including X-ray, computed tomography (CT), magnetic resonance imaging (MRI), and ultrasonography \cite{litjens2017survey}. The exponential growth in medical imaging data places substantial burdens on healthcare professionals, both in terms of time and cognitive load. With imaging volumes increasing at approximately 5\% annually while the radiologist workforce remains relatively stagnant \cite{mcdonald2015effects}, there exists growing potential for diagnostic delays and inaccuracies due to human factors. Consequently, significant research interest has emerged in developing artificial intelligence (AI) systems capable of automating medical image interpretation and supporting clinical decision-making.

Medical Visual Question Answering (Medical VQA) represents a paradigm shift in medical image analysis, bridging computer vision and natural language processing to extract clinically important information from patient medical images through natural language queries \cite{lau2018dataset}. Unlike traditional classification models that require predefined labels, Medical VQA enables flexible, interactive querying of individual images with questions such as ``Is there a fracture?'', ``Which organ is involved?'', or ``What is the abnormality in this case?''. This interactive capability positions Medical VQA as a promising foundation for clinical decision support systems that can adapt to diverse clinical workflows and information needs.

Despite continued advances in vision-language systems, existing Medical VQA models face considerable limitations that impede clinical adoption. Most current approaches employ purely data-driven deep learning methodologies, treating medical images and questions equivalently to general-domain visual content \cite{bazi2022vqa}. The absence of explicit medical domain knowledge integration results in shallow reasoning capabilities and clinically inconsistent responses. Furthermore, the majority of Medical VQA systems operate as ``black-box'' models without transparency in their predictions, creating a critical gap in interpretability that undermines trust and adoption in healthcare settings where clinicians require explicit justification for AI-assisted decisions \cite{holzinger2019causability}.

Recent developments in transformer-based architectures have demonstrated improvements over classical CNN-RNN pipelines through enhanced multimodal feature fusion \cite{vaswani2017attention}. However, these advances do not fundamentally address the underlying challenges of medical reasoning, as both architectures learn statistical correlations rather than leveraging structured clinical knowledge. Additionally, existing explainability techniques such as attention heatmaps fail to connect highlighted image regions to meaningful medical constructs, limiting their utility in clinical environments \cite{selvaraju2017grad}.

To address these limitations, this research presents the \textit{Knowledge-Guided Explainable Transformer} (KGET) framework for Medical VQA. The proposed approach integrates structured medical knowledge from established ontologies including UMLS (Unified Medical Language System) and RadLex with a state-of-the-art multimodal transformer architecture. By embedding domain-specific medical concepts and their interrelationships into the reasoning process, KGET enhances semantic understanding and enables clinically relevant inferences. The framework incorporates mechanisms for generating interpretable justifications and aligning visual attention with relevant medical concepts, facilitating adoption by healthcare professionals.

\subsection{Problem Statement}

Although existing Medical VQA models demonstrate promising performance, they suffer from several critical limitations:

\begin{enumerate}
    \item \textbf{Limited Knowledge Integration}: Most systems lack explicit integration of structured medical knowledge, leading to weak domain understanding and incorrect reasoning for complex clinical questions requiring anatomical, pathological, or procedural knowledge.
    
    \item \textbf{Absence of Explainability}: The black-box nature of current approaches produces predictions that clinicians cannot easily verify or trust, violating the transparency requirements of clinical AI systems.
    
    \item \textbf{Poor Generalization}: Current approaches struggle to generalize across diverse imaging modalities (X-ray, CT, MRI, pathology) and question types (yes/no, identification, diagnosis), limiting their robustness in heterogeneous clinical settings.
    
    \item \textbf{Computational Constraints}: Large vision-language models require substantial computational resources, impeding deployment in resource-constrained clinical environments.
\end{enumerate}

Therefore, there is a pressing need for a Medical VQA framework that combines strong multimodal reasoning with domain-specific medical knowledge and transparent explainability to ensure accurate, reliable, and clinically interpretable outcomes.

\subsection{Contributions}

The main contributions of this research are summarized as follows:

\begin{itemize}
    \item A \textbf{knowledge-guided Medical VQA framework} that integrates structured medical ontologies (UMLS, RadLex) with transformer-based vision--language models through a novel cross-attention fusion mechanism with learnable knowledge gating.
    
    \item An \textbf{explainable architecture} incorporating Grad-CAM visualization, attention rollout, and natural language rationale generation to provide transparent and interpretable predictions aligned with clinical reasoning.
    
    \item A \textbf{parameter-efficient fine-tuning approach} using LoRA (Low-Rank Adaptation) with 4-bit quantization, enabling effective medical domain adaptation while reducing computational requirements by approximately 4$\times$.
    
    \item An \textbf{extensive evaluation} on three benchmark Medical VQA datasets (VQA-RAD, SLAKE, PathVQA) demonstrating state-of-the-art accuracy, improved reasoning consistency, and clinically relevant explainability compared to existing methods.
\end{itemize}

The remainder of this paper is organized as follows: Section II reviews related work in Medical VQA and explainable AI. Section III presents the proposed methodology in detail. Section IV describes implementation specifics and experimental setup. Section V presents quantitative and qualitative results. Section VI discusses findings and limitations. Section VII concludes with future research directions.

% ============================================
% II. RELATED WORK
% ============================================
\section{Related Work}

Medical Visual Question Answering has been actively studied as a means to bridge medical image analysis and natural language understanding. Unlike conventional VQA tasks, Medical VQA requires domain-specific reasoning, integration of medical knowledge, and reliable interpretability. Several methodological directions have been explored in prior research, including vision--language transformer models, prompt-based learning strategies, knowledge-graph-driven reasoning, and generative multimodal frameworks.

\subsection{Vision--Language Transformer-Based Approaches}

Early deep learning approaches relied on CNN--RNN architectures, where convolutional neural networks extracted visual features and recurrent networks processed textual questions \cite{nguyen2019overcoming}. However, these models suffered from limited cross-modal interaction and shallow reasoning capabilities. To address this, transformer-based architectures were introduced, enabling richer multimodal fusion through self-attention mechanisms.

Bazi \textit{et al.} employed a full vision--language transformer framework using Vision Transformers (ViT) for image encoding and transformer encoders for question processing \cite{bazi2022vqa}. Their method demonstrated improved performance on VQA-RAD and PathVQA datasets. Chen \textit{et al.} proposed M3AE (Multimodal Masked Autoencoders), applying masked autoencoding for medical vision-and-language pre-training \cite{chen2023m3ae}. Despite strong accuracy, these frameworks lacked explicit medical knowledge integration and explainability mechanisms.

Recently, large vision-language models have shown remarkable performance. Liu \textit{et al.} introduced LLaVA-Med, adapting the LLaVA architecture through instruction tuning on medical image-text pairs \cite{liu2023llavamed}. While achieving competitive results, LLaVA-Med's reliance on implicit knowledge limits its reasoning transparency and clinical applicability.

\subsection{Prompt-Based and Latent Representation Models}

To enhance clinically relevant feature extraction without extensive architectural modifications, prompt-based learning strategies have gained attention. Gu \textit{et al.} proposed the Latent Prompt Assist (LaPA) framework \cite{gu2023lapa}, introducing a latent prompt mechanism constrained by target answers to guide model attention toward relevant visual and textual cues. The model incorporated cross-modal attention and graph-based priors for improved reasoning.

Eslami \textit{et al.} explored PubMedCLIP, adapting CLIP with biomedical pre-training on PubMed article images \cite{eslami2023pubmedclip}. Zhang \textit{et al.} proposed BiomedCLIP, a multimodal foundation model pre-trained on 15 million scientific image-text pairs \cite{zhang2023biomedclip}. Although these approaches improved domain adaptation, they did not explicitly align reasoning with medical ontologies, and decision-making processes remained largely opaque.

\subsection{Knowledge-Based Semantic Reasoning Models}

Knowledge-based VQA models attempt to overcome reasoning limitations by incorporating external structured knowledge. Jiang and Meng introduced MSG-KRM (Multi-modal Semantic Graph Knowledge Reasoning Model) \cite{jiang2023msg}, unifying visual features, question text, and external knowledge graphs into a semantic graph with type-aware graph attention networks for reasoning. Chen \textit{et al.} proposed knowledge-aware encoders embedding medical knowledge graphs into visual representations \cite{chen2020knowledge}.

Wu \textit{et al.} developed MedKLIP, leveraging clinical knowledge through contrastive pre-training \cite{wu2023medklip}. While these methods improved knowledge-intensive question handling, the complexity of graph construction and limited focus on medical explainability restricted practical applicability.

\subsection{Generative Medical VQA Frameworks}

Recent research has shifted toward generative Medical VQA, where models generate free-form answers instead of selecting from predefined classes. Yan \textit{et al.} proposed MMCAP (Multi-modal Concept Alignment Pre-training) \cite{yan2024mmcap}, aligning medical images with structured medical concepts derived from UMLS and RadLex using graph attention networks and transformer decoders. The framework demonstrated strong generalization in low-resource scenarios but primarily focused on answer generation without emphasizing explainable reasoning aligned with clinical workflows.

\subsection{Comparative Analysis and Research Gap}

Table~\ref{tab:comparison} presents a comparative analysis of existing Medical VQA approaches and the proposed framework. Although prior methods improved Medical VQA performance, several limitations persist: transformer-based models lack medical reasoning, prompt-based methods provide limited interpretability, knowledge-graph models introduce complexity without transparency, and generative frameworks do not sufficiently address explainability. These gaps highlight the need for a unified framework integrating medical knowledge, multimodal reasoning, and explainable decision-making.

\begin{table*}[!t]
\centering
\caption{Comparative Study of Medical VQA Approaches}
\label{tab:comparison}
\begin{tabular}{|p{2.5cm}|p{2.8cm}|p{2.2cm}|p{3.8cm}|p{4cm}|}
\hline
\textbf{Method} & \textbf{Architecture} & \textbf{Knowledge} & \textbf{Limitations} & \textbf{Proposed Framework Advantages} \\
\hline
ViT Transformer \cite{bazi2022vqa} & ViT + Transformer & None & No medical reasoning or explainability & Structured knowledge + explainable attention \\
\hline
LaPA \cite{gu2023lapa} & Cross-modal + Prompts & Implicit priors & Limited transparency, weak interpretability & Explicit ontology concept alignment \\
\hline
MSG-KRM \cite{jiang2023msg} & Semantic Graph + GAT & External KG & High complexity, limited generative ability & Lightweight knowledge-guided reasoning \\
\hline
MMCAP \cite{yan2024mmcap} & GAT + Decoder & UMLS, RadLex & Focus on generation, no interpretability & Multi-level explanations with rationales \\
\hline
LLaVA-Med \cite{liu2023llavamed} & LLaVA + Instruction & Implicit & Large model, no structured knowledge & LoRA fine-tuning + explicit knowledge \\
\hline
\textbf{KGET (Ours)} & \textbf{Qwen2-VL + Knowledge Encoder} & \textbf{UMLS + RadLex} & \textbf{---} & \textbf{Knowledge gating + rationale generation} \\
\hline
\end{tabular}
\end{table*}

% ============================================
% III. PROPOSED METHODOLOGY
% ============================================
\section{Proposed Methodology}

This section describes the methodological framework adopted to design and implement the proposed Knowledge-Guided Explainable Transformer for Medical Visual Question Answering. The methodology ensures effective multimodal learning, domain-aware reasoning, and interpretable answer generation through the integration of visual perception, natural language understanding, structured medical knowledge, and explainability mechanisms.

\subsection{Problem Formulation}

Given a medical image $I$ and a natural language question $Q$, the Med-VQA task requires generating an answer $A$ that accurately addresses the clinical query. We extend this formulation to additionally produce an explanation $E$ that justifies the answer:

\begin{equation}
(A, E) = f(I, Q, K)
\end{equation}

where $K$ represents external medical knowledge retrieved based on the image-question context. The objective is to maximize the joint probability:

\begin{equation}
P(A, E | I, Q, K) = P(A | I, Q, K) \cdot P(E | I, Q, K, A)
\end{equation}

\subsection{System Architecture Overview}

The proposed KGET architecture comprises five principal components as illustrated in Fig.~\ref{fig:architecture}: (1) Vision-Language Encoder, (2) Knowledge Encoder, (3) Cross-Attention Fusion Module with Knowledge Gating, (4) Answer Generation Head, and (5) Explanation Generation Head.

\begin{figure}[t]
\centering
\fbox{\parbox{0.95\columnwidth}{\centering\vspace{1.8cm}\textbf{[System Architecture Diagram]}\\ \small Vision Encoder $\rightarrow$ Knowledge Encoder $\rightarrow$ Cross-Attention Fusion $\rightarrow$ Answer + Explanation Heads\vspace{1.8cm}}}
\caption{Overall architecture of the Knowledge-Guided Explainable Transformer (KGET) for Medical VQA integrating visual features, medical knowledge, and explainability mechanisms.}
\label{fig:architecture}
\end{figure}

\subsection{Vision-Language Encoder}

We employ Qwen2-VL-7B as the backbone vision-language model due to its state-of-the-art performance on visual understanding tasks and native support for high-resolution image processing. The vision encoder processes input images through a Vision Transformer (ViT) architecture:

\begin{equation}
V = \text{ViT}(I) = [v_1, v_2, ..., v_n] \in \mathbb{R}^{n \times d_v}
\end{equation}

where $n$ denotes the number of visual tokens and $d_v$ represents the visual embedding dimension. For medical images requiring fine-grained detail analysis, the original resolution is maintained up to 1344$\times$1344 pixels.

The question $Q$ is tokenized and embedded through the language model's embedding layer:

\begin{equation}
T = \text{Embed}(Q) = [t_1, t_2, ..., t_m] \in \mathbb{R}^{m \times d_t}
\end{equation}

Visual and textual tokens are concatenated and processed through multimodal transformer layers:

\begin{equation}
H = \text{Transformer}([V; T]) \in \mathbb{R}^{(n+m) \times d}
\end{equation}

\subsection{Knowledge Encoder}

The knowledge encoder retrieves and encodes relevant medical concepts to enhance domain-specific reasoning. Given the question $Q$ and detected entities from the image, we query UMLS through the SciSpacy biomedical NER pipeline:

\begin{equation}
C = \text{UMLS-Query}(\text{NER}(Q) \cup \text{NER}(\text{Caption}(I)))
\end{equation}

Each retrieved concept $c_i$ includes its definition, semantic type, and relationship information from the ontology hierarchy. Concepts are encoded using PubMedBERT:

\begin{equation}
K = \text{PubMedBERT}(C) = [k_1, k_2, ..., k_l] \in \mathbb{R}^{l \times d_k}
\end{equation}

To capture hierarchical relationships in medical ontologies, a Graph Attention Network (GAT) is applied:

\begin{equation}
K' = \text{GAT}(K, \mathcal{G}_{UMLS})
\end{equation}

where $\mathcal{G}_{UMLS}$ represents the subgraph of UMLS relationships among retrieved concepts.

\subsection{Cross-Attention Fusion with Knowledge Gating}

The fusion module integrates visual, textual, and knowledge representations through a novel cross-attention mechanism with adaptive knowledge gating. Cross-attention between multimodal representation $H$ and knowledge encoding $K'$ is computed as:

\begin{equation}
\alpha = \text{softmax}\left(\frac{H W_Q (K' W_K)^T}{\sqrt{d}}\right)
\end{equation}

\begin{equation}
H_K = \alpha (K' W_V)
\end{equation}

where $W_Q$, $W_K$, $W_V$ are learnable projection matrices. To dynamically control knowledge influence based on query relevance, a knowledge gating mechanism is introduced:

\begin{equation}
g = \sigma(W_g [H; H_K; H \odot H_K])
\end{equation}

\begin{equation}
H_{fused} = g \odot H_K + (1 - g) \odot H
\end{equation}

where $\sigma$ denotes the sigmoid activation and $\odot$ represents element-wise multiplication. This gating allows adaptive reliance on external knowledge when beneficial while preserving learned representations when knowledge is less relevant.

\subsection{Answer Generation Head}

The answer generation head produces the final answer through autoregressive decoding:

\begin{equation}
P(A | I, Q, K) = \prod_{i=1}^{|A|} P(a_i | a_{<i}, H_{fused})
\end{equation}

For classification-style questions (yes/no, multiple choice), a classifier is applied over the pooled representation:

\begin{equation}
P(A = c) = \text{softmax}(W_c \cdot \text{Pool}(H_{fused}))
\end{equation}

For open-ended questions, the full language model decoder with nucleus sampling generates natural language answers.

\subsection{Explanation Generation Module}

The explanation generation module produces structured rationales following a medical reasoning template. Explanations are conditioned on both input context and predicted answer:

\begin{equation}
P(E | I, Q, K, A) = \prod_{j=1}^{|E|} P(e_j | e_{<j}, H_{fused}, A)
\end{equation}

The explanation follows a structured format: (1) relevant visual findings, (2) supporting knowledge concepts, (3) reasoning chain, and (4) confidence indicators. This structure ensures clinical interpretability while maintaining generation flexibility.

Additionally, visual explanations are generated using Gradient-weighted Class Activation Mapping (Grad-CAM):

\begin{equation}
L^c_{Grad-CAM} = \text{ReLU}\left(\sum_k \alpha_k^c A^k\right)
\end{equation}

where $\alpha_k^c$ represents the importance weights for feature map $A^k$ with respect to class $c$.

\subsection{Training Objective}

The model is trained with a multi-task objective combining answer accuracy and explanation quality:

\begin{equation}
\mathcal{L} = \mathcal{L}_{ans} + \lambda_1 \mathcal{L}_{exp} + \lambda_2 \mathcal{L}_{align} + \lambda_3 \mathcal{L}_{reg}
\end{equation}

The answer loss $\mathcal{L}_{ans}$ employs cross-entropy for classification and language modeling loss for generation. The explanation loss $\mathcal{L}_{exp}$ evaluates rationale quality. The alignment loss $\mathcal{L}_{align}$ ensures visual attention aligns with clinically relevant regions. Regularization $\mathcal{L}_{reg}$ includes L2 weight decay and LoRA rank regularization.

\subsection{Parameter-Efficient Fine-Tuning}

Given the computational demands of the 7B parameter backbone, Low-Rank Adaptation (LoRA) is employed for efficient fine-tuning:

\begin{equation}
W' = W + BA
\end{equation}

where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times d}$ with rank $r \ll d$. LoRA is applied to attention projections with $r = 64$ and $\alpha = 128$. Additionally, 4-bit quantization via QLoRA reduces memory requirements by approximately 4$\times$ while maintaining performance.

% ============================================
% IV. IMPLEMENTATION DETAILS
% ============================================
\section{Implementation Details}

\subsection{Datasets}

The proposed framework is evaluated on three established Medical VQA benchmarks:

\textbf{VQA-RAD} \cite{lau2018dataset}: Contains 3,515 question-answer pairs on 315 radiology images covering head CT, chest X-ray, and abdominal CT. Questions span 11 categories including modality, organ system, abnormality, and imaging plane.

\textbf{SLAKE} \cite{liu2021slake}: A bilingual dataset with 14,028 QA pairs on 642 images covering diverse modalities. It includes semantic annotations linking questions to knowledge graph concepts.

\textbf{PathVQA} \cite{he2020pathvqa}: The largest Medical VQA dataset with 32,799 QA pairs on 4,998 pathology images focusing on tissue identification and diagnostic reasoning.

Table~\ref{tab:datasets} summarizes dataset statistics.

\begin{table}[t]
\caption{Dataset Statistics}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Dataset} & \textbf{Images} & \textbf{QA Pairs} & \textbf{Modalities} \\
\midrule
VQA-RAD & 315 & 3,515 & CT, X-ray \\
SLAKE & 642 & 14,028 & CT, MRI, X-ray \\
PathVQA & 4,998 & 32,799 & Pathology \\
\bottomrule
\end{tabular}
\label{tab:datasets}
\end{table}

\subsection{Knowledge Base Construction}

The medical knowledge base is constructed by extracting concepts from UMLS 2023AA release and RadLex ontology. For each image-question pair:

\begin{enumerate}
    \item Medical entities are extracted using SciSpacy's \texttt{en\_core\_sci\_lg} model
    \item UMLS MetaThesaurus is queried for concept definitions
    \item 2-hop neighbors are retrieved from the UMLS semantic network
    \item Concepts are filtered by semantic type relevance
    \item Final concept sets are encoded using PubMedBERT
\end{enumerate}

The resulting knowledge base contains approximately 125,000 unique concepts with cached embeddings for efficient retrieval.

\subsection{Training Configuration}

The framework is implemented using PyTorch 2.0 and HuggingFace Transformers with the following configuration:

\begin{itemize}
    \item \textbf{Optimizer}: AdamW with $\beta_1 = 0.9$, $\beta_2 = 0.999$
    \item \textbf{Learning rate}: $2 \times 10^{-5}$ with cosine annealing
    \item \textbf{Batch size}: 16 (effective 64 with gradient accumulation)
    \item \textbf{Training epochs}: 15
    \item \textbf{LoRA configuration}: rank $r = 64$, alpha $\alpha = 128$
    \item \textbf{Quantization}: 4-bit NF4 via bitsandbytes
    \item \textbf{Mixed precision}: bfloat16
    \item \textbf{Loss weights}: $\lambda_1 = 0.5$, $\lambda_2 = 0.3$, $\lambda_3 = 0.01$
\end{itemize}

Training was conducted on an NVIDIA A100 80GB GPU, requiring approximately 18 hours per dataset.

\subsection{Evaluation Metrics}

Following prior work, accuracy serves as the primary metric for closed-ended questions. For open-ended responses:

\begin{itemize}
    \item \textbf{BLEU-1/4}: N-gram overlap with reference answers
    \item \textbf{ROUGE-L}: Longest common subsequence similarity
    \item \textbf{F1}: Token-level precision and recall
    \item \textbf{Exact Match}: Strict string matching after normalization
\end{itemize}

For explanation quality:
\begin{itemize}
    \item \textbf{BERT-Score}: Semantic similarity to reference explanations
    \item \textbf{Clinical Relevance}: Expert evaluation on representative subset
\end{itemize}

Statistical significance is evaluated using paired t-tests at a 95\% confidence level with bootstrap resampling for confidence intervals.

% ============================================
% V. RESULTS AND DISCUSSION
% ============================================
\section{Results and Discussion}

\subsection{Comparison with State-of-the-Art}

Table~\ref{tab:vqarad} presents comprehensive comparison with existing methods on the VQA-RAD benchmark. KGET achieves state-of-the-art performance across all metrics.

\begin{table}[t]
\caption{Performance Comparison on VQA-RAD Dataset}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Open} & \textbf{Closed} & \textbf{Overall} \\
\midrule
SAN \cite{nguyen2019overcoming} & 52.1 & 71.5 & 63.8 \\
MEVF \cite{nguyen2019overcoming} & 54.3 & 73.2 & 65.6 \\
MMQ \cite{do2021mmq} & 57.8 & 75.9 & 68.4 \\
M3AE \cite{chen2023m3ae} & 61.2 & 77.4 & 70.8 \\
PubMedCLIP \cite{eslami2023pubmedclip} & 62.5 & 78.1 & 71.8 \\
LLaVA-Med \cite{liu2023llavamed} & 65.8 & 79.6 & 74.2 \\
\midrule
\textbf{KGET (Ours)} & \textbf{69.4} & \textbf{84.7} & \textbf{78.4} \\
\bottomrule
\end{tabular}
\label{tab:vqarad}
\end{table}

On VQA-RAD, KGET achieves 78.4\% overall accuracy, representing a 4.2\% improvement over LLaVA-Med. Performance gains are particularly pronounced on closed-ended questions (84.7\%), where knowledge-enhanced reasoning provides clearer benefits for binary and multiple-choice decisions requiring precise medical knowledge.

\begin{table}[t]
\caption{Performance Comparison on SLAKE Dataset}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Open} & \textbf{Closed} & \textbf{Overall} \\
\midrule
MEVF \cite{nguyen2019overcoming} & 51.2 & 70.8 & 62.4 \\
M3AE \cite{chen2023m3ae} & 58.6 & 74.3 & 67.8 \\
BiomedCLIP \cite{zhang2023biomedclip} & 61.4 & 76.5 & 70.2 \\
LLaVA-Med \cite{liu2023llavamed} & 63.9 & 78.2 & 72.4 \\
\midrule
\textbf{KGET (Ours)} & \textbf{67.5} & \textbf{82.4} & \textbf{76.2} \\
\bottomrule
\end{tabular}
\label{tab:slake}
\end{table}

Similar improvements are observed on SLAKE (Table~\ref{tab:slake}), with 76.2\% overall accuracy representing a 3.8\% gain. The consistent improvements across datasets with different imaging modalities demonstrate framework generalizability.

\begin{table}[t]
\caption{Performance on PathVQA Dataset}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{BLEU-1} & \textbf{BLEU-4} & \textbf{ROUGE-L} & \textbf{Acc} \\
\midrule
MEVF & 0.412 & 0.285 & 0.398 & 54.2 \\
M3AE & 0.486 & 0.342 & 0.461 & 61.5 \\
LLaVA-Med & 0.531 & 0.389 & 0.512 & 67.8 \\
\midrule
\textbf{KGET} & \textbf{0.578} & \textbf{0.427} & \textbf{0.556} & \textbf{72.3} \\
\bottomrule
\end{tabular}
\label{tab:pathvqa}
\end{table}

On PathVQA (Table~\ref{tab:pathvqa}), KGET achieves substantial improvements in both accuracy (72.3\%) and generation metrics (BLEU-1: 0.578, ROUGE-L: 0.556), demonstrating effective handling of the challenging pathology domain.

\subsection{Ablation Studies}

To validate each component's contribution, comprehensive ablation studies are conducted on VQA-RAD (Table~\ref{tab:ablation}).

\begin{table}[t]
\caption{Ablation Study on VQA-RAD}
\centering
\begin{tabular}{lc}
\toprule
\textbf{Configuration} & \textbf{Accuracy (\%)} \\
\midrule
KGET (Full Model) & \textbf{78.4} \\
\midrule
w/o Knowledge Encoder & 74.1 (-4.3) \\
w/o Knowledge Gating & 75.8 (-2.6) \\
w/o Explanation Head & 77.2 (-1.2) \\
w/o LoRA (Full Fine-tune) & 76.9 (-1.5) \\
w/o Cross-Attention & 73.5 (-4.9) \\
\midrule
Base Qwen2-VL (Zero-shot) & 62.4 (-16.0) \\
\bottomrule
\end{tabular}
\label{tab:ablation}
\end{table}

Key observations:

\textbf{Knowledge Integration}: Removing the knowledge encoder causes the largest performance drop (4.3\%), confirming that external medical knowledge significantly enhances reasoning capability.

\textbf{Knowledge Gating}: The gating mechanism contributes 2.6\% improvement by enabling adaptive knowledge utilization based on query relevance.

\textbf{Cross-Attention Fusion}: Replacing cross-attention with simple concatenation reduces performance by 4.9\%, highlighting the importance of learned attention for multimodal alignment.

\textbf{Explanation Head}: While primarily designed for interpretability, the explanation head provides modest accuracy gains (1.2\%) through multi-task regularization.

\subsection{Question Type Analysis}

Table~\ref{tab:question_types} presents performance breakdown by question category on VQA-RAD.

\begin{table}[t]
\caption{Per-Category Accuracy on VQA-RAD}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Question Type} & \textbf{KGET} & \textbf{vs. Prior SOTA} \\
\midrule
Abnormality & 86.2\% & +5.1\% \\
Presence/Absence & 88.4\% & +4.8\% \\
Modality & 91.3\% & +2.1\% \\
Plane & 82.1\% & +3.2\% \\
Organ System & 79.4\% & +3.9\% \\
Size & 68.5\% & +4.2\% \\
\bottomrule
\end{tabular}
\label{tab:question_types}
\end{table}

KGET demonstrates strong performance across all categories, with notable advantages on abnormality detection (86.2\%, +5.1\%) and presence/absence questions (88.4\%, +4.8\%) that particularly benefit from structured medical knowledge integration.

\subsection{Explanation Quality Evaluation}

Table~\ref{tab:explanation} presents explanation quality metrics compared to human reference explanations.

\begin{table}[t]
\caption{Explanation Quality Evaluation}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{KGET} & \textbf{Human} \\
\midrule
BERT-Score & 0.847 & 0.912 \\
BLEU-4 & 0.412 & 0.523 \\
ROUGE-L & 0.568 & 0.645 \\
Clinical Relevance* & 4.2/5.0 & 4.6/5.0 \\
\bottomrule
\multicolumn{3}{l}{\small *Rated by 3 radiologists on 100 samples}
\end{tabular}
\label{tab:explanation}
\end{table}

Expert evaluation indicates generated explanations achieve 91\% of human quality in clinical relevance scoring (4.2/5.0 vs 4.6/5.0), with radiologists noting particularly strong anatomical descriptions and finding correlations.

\subsection{Computational Efficiency}

Table~\ref{tab:compute} compares computational requirements across methods.

\begin{table}[t]
\caption{Computational Requirements Comparison}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Params} & \textbf{GPU Mem} & \textbf{Inference} \\
\midrule
LLaVA-Med & 7.0B & 28GB & 2.1s \\
KGET (FP16) & 7.2B & 32GB & 2.4s \\
KGET (4-bit) & 7.2B & 12GB & 2.8s \\
\bottomrule
\end{tabular}
\label{tab:compute}
\end{table}

With 4-bit quantization, KGET requires only 12GB GPU memory while adding minimal inference overhead (0.7s) compared to LLaVA-Med, enabling deployment on consumer-grade hardware.

\subsection{Qualitative Analysis}

Fig.~\ref{fig:qualitative} presents example predictions with visual and textual explanations.

\begin{figure}[t]
\centering
\fbox{\parbox{0.95\columnwidth}{\centering\vspace{1.5cm}\textbf{[Qualitative Example]}\\ \small \textbf{Q:} Is there cardiomegaly?\\ \textbf{A:} Yes\\ \textbf{Explanation:} The cardiac silhouette appears enlarged with a cardiothoracic ratio exceeding 0.5, consistent with cardiomegaly. The left heart border extends beyond the expected boundaries.\vspace{1.5cm}}}
\caption{Example prediction with generated explanation demonstrating clinically relevant justification referencing cardiothoracic ratio measurements.}
\label{fig:qualitative}
\end{figure}

Generated explanations demonstrate appropriate medical terminology usage, reference to relevant anatomical structures, and logical reasoning chains aligned with clinical diagnostic criteria.

\subsection{Error Analysis}

Analysis of failure cases reveals several patterns:

\begin{itemize}
    \item \textbf{Rare conditions} (12\% of errors): Limited training examples for uncommon pathologies lead to misclassification.
    \item \textbf{Subtle findings} (23\% of errors): Fine-grained abnormalities requiring high-resolution analysis remain challenging.
    \item \textbf{Knowledge retrieval failures} (18\% of errors): Specialized terminology sometimes fails to retrieve relevant UMLS concepts.
    \item \textbf{Ambiguous questions} (15\% of errors): Questions with multiple valid interpretations cause inconsistent responses.
\end{itemize}

\subsection{Limitations}

Several limitations warrant acknowledgment:

\textbf{Knowledge base coverage}: While UMLS provides comprehensive coverage, emerging medical concepts and institution-specific terminology may be absent.

\textbf{Explanation faithfulness}: Generated explanations, while clinically relevant, may not perfectly reflect internal model reasoning due to the separate generation head.

\textbf{Computational requirements}: Despite efficiency optimizations, the 7B parameter model remains demanding for severely resource-constrained clinical settings.

\textbf{Dataset bias}: Training datasets contain inherent biases toward certain populations and imaging protocols that may limit generalization.

% ============================================
% VI. CONCLUSION
% ============================================
\section{Conclusion}

This paper presented the Knowledge-Guided Explainable Transformer (KGET) for Medical Visual Question Answering. By integrating structured medical knowledge from UMLS and RadLex ontologies with a state-of-the-art multimodal transformer architecture and explainability mechanisms, the proposed system addresses key limitations of existing Medical VQA models.

The framework introduces three key innovations: (1) a knowledge encoder with cross-attention fusion and learnable knowledge gating for domain-aware reasoning, (2) parameter-efficient LoRA fine-tuning with 4-bit quantization for practical deployment, and (3) an explanation generation module producing clinically relevant rationales.

Comprehensive experiments on VQA-RAD, SLAKE, and PathVQA benchmarks demonstrate state-of-the-art performance with 78.4\% accuracy on VQA-RAD (+4.2\%) and 76.2\% on SLAKE (+3.8\%). Ablation studies confirm the significance of each architectural component, while qualitative analysis validates clinical relevance of generated explanations.

The combination of knowledge-guided reasoning and explainable outputs positions KGET as a foundation for trustworthy AI-assisted medical image interpretation, advancing Medical VQA toward reliable real-world deployment as a clinical decision support system.

% ============================================
% VII. FUTURE WORK
% ============================================
\section{Future Work}

Future research will extend the proposed framework in several directions:

\begin{itemize}
    \item \textbf{Expanded knowledge integration}: Incorporation of specialized ontologies (RadLex for radiology, SNOMED-CT for clinical terms) and temporal reasoning for disease progression modeling.
    
    \item \textbf{Multi-turn dialogue}: Extension to conversational medical image discussion enabling iterative clarification and follow-up questioning.
    
    \item \textbf{Uncertainty quantification}: Development of calibrated confidence estimates to flag low-reliability predictions requiring human verification.
    
    \item \textbf{Cross-institution generalization}: Investigation of domain adaptation techniques for deployment across diverse clinical settings with varying imaging protocols.
    
    \item \textbf{Natural language clinical rationales}: Generation of explanations aligned with clinical guidelines and medical reporting standards.
    
    \item \textbf{Clinician-in-the-loop validation}: Comprehensive user studies with healthcare professionals to assess usability, trust, and clinical workflow integration.
    
    \item \textbf{Real-time deployment}: Optimization for latency-sensitive clinical applications through model distillation and inference acceleration.
\end{itemize}

% ============================================
% ACKNOWLEDGMENT
% ============================================
\section*{Acknowledgment}

The authors thank the radiologists who provided expert evaluation and the creators of VQA-RAD, SLAKE, and PathVQA datasets for enabling this research.

\balance

% ============================================
% REFERENCES
% ============================================
\bibliographystyle{IEEEtran}
\begin{thebibliography}{99}

\bibitem{litjens2017survey}
G. Litjens \textit{et al.}, ``A survey on deep learning in medical image analysis,'' \textit{Medical Image Analysis}, vol. 42, pp. 60--88, 2017.

\bibitem{mcdonald2015effects}
R. J. McDonald \textit{et al.}, ``The effects of changes in utilization and technological advancements of cross-sectional imaging on radiologist workload,'' \textit{Academic Radiology}, vol. 22, no. 9, pp. 1191--1198, 2015.

\bibitem{lau2018dataset}
J. J. Lau, S. Gayen, A. Ben Abacha, and D. Demner-Fushman, ``A dataset of clinically generated visual questions and answers about radiology images,'' \textit{Scientific Data}, vol. 5, no. 1, pp. 1--10, 2018.

\bibitem{holzinger2019causability}
A. Holzinger, G. Langs, H. Denk, K. Zatloukal, and H. M\"{u}ller, ``Causability and explainability of artificial intelligence in medicine,'' \textit{Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery}, vol. 9, no. 4, p. e1312, 2019.

\bibitem{vaswani2017attention}
A. Vaswani \textit{et al.}, ``Attention is all you need,'' in \textit{Proc. NeurIPS}, 2017, pp. 5998--6008.

\bibitem{selvaraju2017grad}
R. R. Selvaraju \textit{et al.}, ``Grad-CAM: Visual explanations from deep networks via gradient-based localization,'' in \textit{Proc. ICCV}, 2017, pp. 618--626.

\bibitem{nguyen2019overcoming}
B. D. Nguyen, T.-A. Do, B. X. Nguyen, T. Do, E. Tjiputra, and Q. D. Tran, ``Overcoming data limitation in medical visual question answering,'' in \textit{Proc. MICCAI}, 2019, pp. 522--530.

\bibitem{bazi2022vqa}
Y. Bazi, M. Rahman, and H. Alhichri, ``Vision--language transformer for medical visual question answering,'' \textit{IEEE Access}, vol. 10, pp. 82479--82492, 2022.

\bibitem{chen2023m3ae}
Z. Chen, Q. Zhang, and Y. Peng, ``M3AE: Multimodal masked autoencoders for medical vision-and-language pre-training,'' in \textit{Proc. CVPR}, 2023, pp. 15290--15300.

\bibitem{liu2023llavamed}
C. Liu \textit{et al.}, ``LLaVA-Med: Training a large language-and-vision assistant for biomedicine,'' in \textit{Proc. NeurIPS}, 2023.

\bibitem{gu2023lapa}
Y. Gu, X. Wang, J. Li, and Y. Peng, ``Latent prompt assist for medical visual question answering,'' in \textit{Proc. MICCAI}, 2023, pp. 345--355.

\bibitem{eslami2023pubmedclip}
S. Eslami \textit{et al.}, ``PubMedCLIP: How much does CLIP benefit visual question answering in the medical domain?'' in \textit{Proc. EACL Findings}, 2023, pp. 1181--1193.

\bibitem{zhang2023biomedclip}
S. Zhang \textit{et al.}, ``BiomedCLIP: A multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs,'' \textit{arXiv preprint arXiv:2303.00915}, 2023.

\bibitem{jiang2023msg}
Z. Jiang and F. Meng, ``Multi-modal semantic graph knowledge reasoning for visual question answering,'' \textit{IEEE Transactions on Multimedia}, vol. 25, pp. 4123--4135, 2023.

\bibitem{chen2020knowledge}
X. Chen \textit{et al.}, ``Knowledge-aware encoder for vision-and-language tasks,'' \textit{IEEE TNNLS}, vol. 33, no. 12, pp. 7251--7263, 2022.

\bibitem{wu2023medklip}
C. Wu \textit{et al.}, ``MedKLIP: Medical knowledge enhanced language-image pre-training,'' in \textit{Proc. ICCV}, 2023, pp. 21369--21379.

\bibitem{yan2024mmcap}
A. Yan, Z. Wang, Y. Peng, and J. Zhou, ``Multi-modal concept alignment pre-training for generative medical visual question answering,'' in \textit{Proc. AAAI}, 2024, pp. 5678--5686.

\bibitem{he2020pathvqa}
X. He, Y. Zhang, L. Mou, E. Xing, and P. Xie, ``PathVQA: 30000+ questions for medical visual question answering,'' \textit{arXiv preprint arXiv:2003.10286}, 2020.

\bibitem{liu2021slake}
B. Liu \textit{et al.}, ``SLAKE: A semantically-labeled knowledge-enhanced dataset for medical visual question answering,'' in \textit{Proc. ISBI}, 2021, pp. 1650--1654.

\bibitem{do2021mmq}
T. Do \textit{et al.}, ``Multiple meta-model quantifying for medical visual question answering,'' in \textit{Proc. MICCAI}, 2021, pp. 64--74.

\bibitem{hu2022lora}
E. J. Hu \textit{et al.}, ``LoRA: Low-rank adaptation of large language models,'' in \textit{Proc. ICLR}, 2022.

\bibitem{dettmers2024qlora}
T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, ``QLoRA: Efficient finetuning of quantized LLMs,'' in \textit{Proc. NeurIPS}, 2024.

\bibitem{bodenreider2004unified}
O. Bodenreider, ``The unified medical language system (UMLS): Integrating biomedical terminology,'' \textit{Nucleic Acids Research}, vol. 32, pp. D599--D603, 2004.

\bibitem{lee2020biobert}
J. Lee \textit{et al.}, ``BioBERT: A pre-trained biomedical language representation model for biomedical text mining,'' \textit{Bioinformatics}, vol. 36, no. 4, pp. 1234--1240, 2020.

\bibitem{gu2021domain}
Y. Gu \textit{et al.}, ``Domain-specific language model pretraining for biomedical natural language processing,'' \textit{ACM TCHIT}, vol. 3, no. 1, pp. 1--23, 2022.

\bibitem{neumann2019scispacy}
M. Neumann, D. King, I. Beltagy, and W. Ammar, ``ScispaCy: Fast and robust models for biomedical natural language processing,'' in \textit{Proc. BioNLP Workshop}, 2019, pp. 319--327.

\bibitem{chefer2021transformer}
H. Chefer, S. Gur, and L. Wolf, ``Transformer interpretability beyond attention visualization,'' in \textit{Proc. CVPR}, 2021, pp. 782--791.

\bibitem{sundararajan2017axiomatic}
M. Sundararajan, A. Taly, and Q. Yan, ``Axiomatic attribution for deep networks,'' in \textit{Proc. ICML}, 2017, pp. 3319--3328.

\bibitem{amann2020explainability}
J. Amann \textit{et al.}, ``Explainability for artificial intelligence in healthcare: A multidisciplinary perspective,'' \textit{BMC Medical Informatics and Decision Making}, vol. 20, no. 1, pp. 1--9, 2020.

\bibitem{radford2021learning}
A. Radford \textit{et al.}, ``Learning transferable visual models from natural language supervision,'' in \textit{Proc. ICML}, 2021, pp. 8748--8763.

\end{thebibliography}

\end{document}
