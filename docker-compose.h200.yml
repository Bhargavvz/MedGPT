# ============================================================================
# Medical VQA - H200 Docker Compose
# Optimized for NVIDIA H200 (141GB) + 1.9TB RAM
# ============================================================================

version: "3.8"

services:
  # ==========================================================================
  # Training Service
  # ==========================================================================
  training:
    build:
      context: .
      dockerfile: Dockerfile.h200
    container_name: medvqa-training
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
      - DEVICE=cuda
      - HF_HOME=/app/models_cache
      - TRANSFORMERS_CACHE=/app/models_cache
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - TOKENIZERS_PARALLELISM=true
      - OMP_NUM_THREADS=16
      - WANDB_PROJECT=medical-vqa-kget
    volumes:
      - ./data:/app/data
      - ./checkpoints:/app/checkpoints
      - ./logs:/app/logs
      - ./models_cache:/app/models_cache
      - ./config:/app/config
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
        limits:
          memory: 256g
    shm_size: '64g'
    ulimits:
      memlock:
        soft: -1
        hard: -1
    command: >
      python training/train.py --config config/h200_config.yaml
    profiles:
      - training

  # ==========================================================================
  # Web Application Service
  # ==========================================================================
  webapp:
    build:
      context: .
      dockerfile: Dockerfile.h200
    container_name: medvqa-webapp
    runtime: nvidia
    ports:
      - "8000:8000"
      - "7860:7860"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
      - DEVICE=cuda
      - MODEL_PATH=/app/checkpoints/best_model
      - HF_HOME=/app/models_cache
      - TRANSFORMERS_CACHE=/app/models_cache
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    volumes:
      - ./checkpoints:/app/checkpoints
      - ./uploads:/app/uploads
      - ./results:/app/results
      - ./models_cache:/app/models_cache
      - ./logs:/app/logs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
        limits:
          memory: 128g
    shm_size: '32g'
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/api/health" ]
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 120s
    restart: unless-stopped
    command: >
      python -m uvicorn webapp.app:app --host 0.0.0.0 --port 8000 --workers 4

  # ==========================================================================
  # Evaluation Service
  # ==========================================================================
  evaluation:
    build:
      context: .
      dockerfile: Dockerfile.h200
    container_name: medvqa-eval
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
      - DEVICE=cuda
      - MODEL_PATH=/app/checkpoints/best_model
      - HF_HOME=/app/models_cache
    volumes:
      - ./data:/app/data
      - ./checkpoints:/app/checkpoints
      - ./results:/app/results
      - ./models_cache:/app/models_cache
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    command: >
      python evaluation/evaluate.py --config config/h200_config.yaml --output_dir results/
    profiles:
      - evaluation

  # ==========================================================================
  # TensorBoard Monitoring
  # ==========================================================================
  tensorboard:
    image: tensorflow/tensorflow:latest
    container_name: medvqa-tensorboard
    ports:
      - "6006:6006"
    volumes:
      - ./logs:/logs
    command: tensorboard --logdir=/logs --host=0.0.0.0 --port=6006
    restart: unless-stopped
    profiles:
      - monitoring
