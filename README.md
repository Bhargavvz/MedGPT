# Knowledge-Guided Explainable Transformer for Medical Visual Question Answering

<div align="center">

![Medical VQA](https://img.shields.io/badge/Medical-VQA-blue?style=for-the-badge)
![Python](https://img.shields.io/badge/Python-3.10+-green?style=for-the-badge&logo=python)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red?style=for-the-badge&logo=pytorch)
![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)

**An AI-powered system for answering questions about medical images with explainable reasoning**

[Features](#features) â€¢ [Installation](#installation) â€¢ [Usage](#usage) â€¢ [Training](#training) â€¢ [API](#api) â€¢ [Evaluation](#evaluation)

</div>

---

## ğŸŒŸ Features

- **ğŸ§  Qwen2-VL-7B Base Model** - State-of-the-art vision-language understanding
- **ğŸ“š Medical Knowledge Integration** - BioBERT/PubMedBERT for domain-specific knowledge
- **ğŸ”¬ Explainable AI** - Grad-CAM, Attention Rollout, Integrated Gradients
- **âš¡ Efficient Fine-tuning** - LoRA/QLoRA with 4-bit quantization
- **ğŸŒ Web Interface** - FastAPI backend with modern HTML/CSS/JS frontend
- **ğŸ³ Docker Ready** - Easy deployment with GPU support

---

## ğŸ“ Project Structure

```
medical-vqa/
â”œâ”€â”€ config/                     # Configuration files
â”‚   â”œâ”€â”€ config.yaml            # Main configuration
â”‚   â””â”€â”€ model_config.py        # Python dataclass configs
â”œâ”€â”€ data/                       # Data handling
â”‚   â”œâ”€â”€ schema.json            # Dataset schema
â”‚   â””â”€â”€ dataset_loader.py      # Data loading utilities
â”œâ”€â”€ preprocess/                 # Preprocessing modules
â”‚   â”œâ”€â”€ dicom_processor.py     # DICOM to PNG conversion
â”‚   â”œâ”€â”€ image_augmentation.py  # Medical image augmentation
â”‚   â”œâ”€â”€ text_processor.py      # Text preprocessing
â”‚   â””â”€â”€ knowledge_retriever.py # UMLS/SciSpacy integration
â”œâ”€â”€ models/                     # Model architecture
â”‚   â”œâ”€â”€ vision_encoder.py      # Vision encoder (CLIP ViT)
â”‚   â”œâ”€â”€ knowledge_encoder.py   # Knowledge encoder (BioBERT)
â”‚   â”œâ”€â”€ fusion_module.py       # Cross-attention fusion
â”‚   â”œâ”€â”€ explanation_head.py    # Rationale generation
â”‚   â””â”€â”€ medical_vqa_model.py   # Main VQA model
â”œâ”€â”€ training/                   # Training pipeline
â”‚   â”œâ”€â”€ loss_functions.py      # Multi-objective losses
â”‚   â”œâ”€â”€ trainer.py             # Custom trainer
â”‚   â””â”€â”€ train.py               # Training script
â”œâ”€â”€ evaluation/                 # Evaluation
â”‚   â”œâ”€â”€ metrics.py             # VQA metrics
â”‚   â””â”€â”€ evaluate.py            # Evaluation pipeline
â”œâ”€â”€ explainability/             # XAI modules
â”‚   â”œâ”€â”€ grad_cam.py            # Grad-CAM implementations
â”‚   â”œâ”€â”€ attention_vis.py       # Attention visualization
â”‚   â””â”€â”€ integrated_gradients.py # Integrated gradients
â”œâ”€â”€ inference/                  # Inference pipeline
â”‚   â””â”€â”€ pipeline.py            # End-to-end inference
â”œâ”€â”€ webapp/                     # Web application
â”‚   â”œâ”€â”€ app.py                 # FastAPI backend
â”‚   â””â”€â”€ static/                # Frontend files
â”œâ”€â”€ Dockerfile                  # Docker configuration
â”œâ”€â”€ docker-compose.yml          # Docker Compose
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # This file
```

---

## ğŸš€ Installation

### Prerequisites

- Python 3.10+
- CUDA 11.8+ (for GPU support)
- 24GB+ VRAM (recommended)

### Quick Start

```bash
# Clone repository
git clone https://github.com/yourusername/medical-vqa.git
cd medical-vqa

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
.\venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt
```

### Docker Setup

```bash
# Build image
docker build -t medical-vqa .

# Run with GPU
docker-compose up -d
```

---

## ğŸ“Š Datasets

### Supported Datasets

| Dataset | Modality | Samples | Task |
|---------|----------|---------|------|
| VQA-RAD | Multi | 3,515 | VQA |
| SLAKE | Multi | 14,028 | VQA |
| PathVQA | Pathology | 32,799 | VQA |
| MedVQA | Multi | 4,706 | VQA |

### Dataset Format

```json
{
  "image": "path/to/image.png",
  "question": "What type of imaging is this?",
  "answer": "chest x-ray",
  "modality": "xray",
  "organ": "lung",
  "disease": "normal",
  "knowledge_snippet": "Chest X-ray is a radiological examination..."
}
```

### Data Preparation

```bash
# Convert DICOM to PNG
python preprocess/dicom_processor.py \
    --input_dir data/raw/dicom \
    --output_dir data/processed/images

# Prepare unified dataset
python scripts/prepare_dataset.py \
    --vqa_rad_path data/raw/vqa-rad \
    --slake_path data/raw/slake \
    --output_path data/processed/unified_vqa.json
```

---

## ğŸ‹ï¸ Training

### Basic Training

```bash
python training/train.py \
    --config config/config.yaml \
    --output_dir ./checkpoints \
    --num_epochs 15 \
    --batch_size 16
```

### Training with DeepSpeed

```bash
deepspeed --num_gpus=1 training/train.py \
    --deepspeed training/deepspeed_config.json \
    --config config/config.yaml
```

### Key Training Arguments

| Argument | Default | Description |
|----------|---------|-------------|
| `--num_epochs` | 15 | Number of training epochs |
| `--batch_size` | 16 | Training batch size |
| `--learning_rate` | 2e-5 | Learning rate |
| `--lora_r` | 64 | LoRA rank |
| `--freeze_vision_epochs` | 3 | Epochs to freeze vision encoder |

---

## ğŸ”® Inference

### Python API

```python
from inference import VQAInference

# Initialize pipeline
pipeline = VQAInference(model_path="./checkpoints/best_model")

# Single prediction
result = pipeline.predict(
    image="xray.png",
    question="What abnormalities are visible?",
    generate_explanation=True,
    generate_heatmap=True
)

print(f"Answer: {result['answer']}")
print(f"Explanation: {result['explanation']}")
```

### REST API

```bash
# Start server
uvicorn webapp.app:app --host 0.0.0.0 --port 8000

# Query endpoint
curl -X POST "http://localhost:8000/api/vqa" \
    -F "image=@xray.png" \
    -F "question=What is the diagnosis?"
```

---

## ğŸŒ Web Application

### Running the Web App

```bash
# Start server
python webapp/app.py

# Access at http://localhost:8000
```

### Features

- ğŸ“¤ Drag-and-drop image upload
- â“ Natural language questions
- ğŸ’¡ Explainable answers with rationale
- ğŸ”¥ Attention heatmap visualization
- ğŸ“¥ Downloadable reports
- ğŸŒ™ Dark mode support

---

## ğŸ“ˆ Evaluation

### Run Evaluation

```bash
python evaluation/evaluate.py \
    --model_path ./checkpoints/best_model \
    --test_file data/test.json \
    --output_dir ./results
```

### Metrics

| Metric | VQA-RAD | SLAKE |
|--------|---------|-------|
| Accuracy | 75%+ | 72%+ |
| BLEU-1 | 0.68 | 0.65 |
| ROUGE-L | 0.71 | 0.68 |

### Ablation Studies

```bash
python evaluation/evaluate.py --run_ablation
```

---

## ğŸ” Explainability

### Generate Explanations

```python
from explainability import GradCAM, AttentionRollout

# Grad-CAM
grad_cam = GradCAM(model.vision_encoder)
heatmap = grad_cam(image_tensor)

# Attention Rollout
rollout = AttentionRollout(model)
attention = rollout(image_tensor)
```

### Visualization

The system provides:
- **Grad-CAM heatmaps** - Highlight important image regions
- **Attention maps** - Token-level attention patterns
- **Textual rationales** - Step-by-step reasoning explanations

---

## âš™ï¸ Configuration

### Model Configuration

```yaml
# config/config.yaml
model:
  base_model: "Qwen/Qwen2-VL-7B-Instruct"
  vision_encoder: "openai/clip-vit-large-patch14"
  knowledge_encoder: "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"

lora:
  enabled: true
  r: 64
  lora_alpha: 128

training:
  num_epochs: 15
  batch_size: 16
  learning_rate: 2e-5
```

---

## ğŸ³ Deployment

### Docker Deployment

```bash
# Build and run
docker-compose up -d

# View logs
docker-compose logs -f

# Stop
docker-compose down
```

### Cloud Deployment

The application is ready for deployment on:
- AWS EC2 (with GPU instances)
- Google Cloud Compute Engine
- Azure Virtual Machines

---

## âš ï¸ Disclaimer

**This system is for research and educational purposes only.** It should NOT be used for clinical diagnosis or medical decision-making. Always consult qualified healthcare professionals for medical advice.

---

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [Qwen-VL](https://github.com/QwenLM/Qwen-VL) - Base vision-language model
- [BioBERT](https://github.com/dmis-lab/biobert) - Biomedical language model
- [VQA-RAD](https://www.nature.com/articles/sdata2018251) - VQA dataset
- [SLAKE](https://github.com/Sadayuki-Sato/SLAKE) - Semantic VQA dataset

---

<div align="center">

**Built with â¤ï¸ for advancing medical AI**

</div>
